---
title             : "Reporting bias, not external focus: A robust Bayesian meta-analysis of the attentional focus literature"
shorttitle        : "Reporting bias, not external focus"
author: 
  - name          : ""

authornote: |
  \vspace{-0.5cm}
  \noindent Data and code: https://osf.io/vfmx2/?view_only=002325d59dd64562a20301167240f0f9

abstract: |
  Evidence has ostensibly been accumulating over the past two decades suggesting that an external focus of attention is superior to an internal focus for the performance and learning of motor skills. Seven previous meta-studies have all reported evidence of external focus superiority---the most comprehensive of which concluded the benefits apply to motor skill (a) retention, (b) transfer, and (c) performance; results in (d) reduced electromyographic activity during performance, and that (e) more distal external foci are superior to proximal external foci for performance. Here, we re-analyzed these data using robust Bayesian meta-analysis methods that included several plausible models of publication bias. We found moderate to strong evidence of publication bias for all five analyses. After correcting for publication bias, estimated mean effects were negligible: *g* = .01 (performance), *g* = .15 (retention), *g* = .09 (transfer), *g* = .06 (electromyography), and *g* = -.01 (distance effect). Bayes factors indicated data favored the null for each analysis, ranging from BF~01~ = 1.3 (retention) to \textcolor{blue}{5.75} (performance). Further, we found clear evidence of heterogeneity in each analysis, suggesting the impact of attentional focus depends on yet unknown contextual factors. Our results contradict the existing consensus that an external focus is always more effective than an internal focus. Instead, focus of attention appears to have a variety of effects that we cannot account for, and on average those effects are small to nil. These results parallel previous metascience suggesting publication bias has obfuscated the motor learning literature.
  
keywords          : |
  Skill acquisition, OPTIMAL theory, Metascience, Heterogeneity, Sport Science
#wordcount         : "XXXX"

bibliography      : ["../references.bib", "../r-references.bib", "../meta-references.bib"]
annotate_references: yes
nocite: |
    @abdollahipour2008-meta,@abdollahipour2020-meta,@abdollahipour2017-meta,@abdollahipour2016-meta,@abdollahipour2014-meta,@abdollahipour2015-meta,@agar2016-meta,@ahmad2013-meta,@asadi2015-meta,@ashraf2017-meta,@beck2017-meta,@beck2018can,@becker2013-meta,@brocken2016-meta,@chiviacowsky2013-meta,@chiviacowsky2010-meta,@chow2014-meta,@christina2014-meta,@coker2016-meta,@coker2018-meta,@teixeira2017-meta,@de2017-meta,@diekfuss2018-meta,@ducharme2015-meta,@ducharme2016-meta,@duke2011-meta,@durham2014-meta,@emanuel2008-meta,@fasoli2002-meta,@flores2015-meta,@freudenheim2010-meta,@gredin2016-meta,@greig2014-meta,@hadler2014-meta,@halperin2017-meta,@halperin2016-meta,@halperin2016b-meta,@hebert2017-meta,@hill2017-meta,@hosseiny2014c-meta,@huang2014-meta,@ille2013-meta,@jackson2011-meta,@jazaeri2018-meta,@kal2013-meta,@kalkhoran2014-meta,@kearney2015-meta,@kim2017-meta,@klostermann2014-meta,@koufou2013-meta,@kuhn2018-meta,@kuhn2017-meta,@land2014-meta,@landers2016-meta,@landers2005-meta,@laufer2007-meta,@lawrence2011-meta,@lisman2013-meta,@lohse2014-meta,@lohse2011-meta,@lohse2012-meta,@lohse2010-meta,@lohse2011b-meta,@makaruk2012-meta,@makaruk2015-meta,@makaruk2013-meta,@marchant2019-meta,@marchant2009-meta,@marchant2017-meta,@marchant2009b-meta,@marchant2018-meta,@maurer2013-meta,@mckay2012-meta,@mcnevin2003-meta,@mcnevin2013-meta,@mornell2019-meta,@mullen2012-meta,@nadzalan2015-meta,@neumann2013-meta,@oki2018-meta,@palmer2017-meta,@parr2009-meta,@pelleck2017-meta,@perkins2003-meta,@perreault2015-meta,@polskaia2015-meta,@porter2011-meta,@porter2013-meta,@porter2012-meta,@porter2010-meta,@porter2013b-meta,@porter2015-meta,@querfurth2016-meta,@raisbeck2019-meta,@richer2017-meta,@rossettini2017-meta,@saemi2016-meta,@saemi2013-meta,@samsudin2017-meta,@schucker2013-meta,@schucker2016-meta,@schucker2009-meta,@schucker2015-meta,@schucker2016b-meta,@schutts2017-meta,@shafizadeh2013-meta,@shafizadeh2013b-meta,@shea1999-meta,@shea1999-meta,@sherwood2014-meta,@stambaugh2017-meta,@stoate2011-meta,@tse2019-meta,@tse2017-meta,@tsetseli2016-meta,@tsetseli2018-meta,@uehara2008-meta,@vance2004-meta,@welling2016-meta,@winkelman2017-meta,@wu2012-meta,@wulf2008-meta,@wulf2010-meta,@wulf2009-meta,@wulf2010b-meta,@wulf1998-meta,@wulf2009b-meta,@wulf1999-meta,@wulf2002-meta,@wulf2001-meta,@wulf2007-meta,@wulf2007b-meta,@wulf2003-meta,@wulf2003b-meta,@wulf2007c-meta,@yogev2017-meta,@zachry2005-meta,@zarghami2012-meta,@zentgraf2009-meta,@ziv2015-meta,@ziv2013-meta

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

#csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
classoption       : "man, donotrepeattitle"
#fontsize          : 11pt
output            :
  papaja::apa6_pdf:
    latex_engine: xelatex

header-includes   :
  # - \usepackage{newtxtext,newtxmath}
  #- \usepackage{fontspec}
  #- \setmainfont{Avenir}
 # - \usepackage{setspace}
 # - \pagewiselinenumbers
  - \raggedbottom
  - \usepackage{sectsty}
  - \sectionfont{\centering\normalfont\normalsize\bfseries}
  - \subsectionfont{\normalfont\normalsize\bfseries\itshape}
  - \subsubsectionfont{\normalfont\normalsize\itshape}
  - \usepackage{censor}
  - \usepackage{graphicx}

  # - \renewcommand\author[1]{}
  # - \renewcommand\affiliation[1]{}
  # - \authorsnames[1, 2, 3, 4, 5, 5, 1, 1]{Brad McKay, Abbey E. Corson, Jeswende Seedu, Celeste S. De Faveri, Hibaa Hasan, Kristen Arnold, Faith C. Adams, Michael J. Carter\vspace{2ex}}
  # - \authorsaffiliations{{Department of Kinesiology, McMaster University}, {School of Human Kinetics, University of Ottawa}, {Faculty of Health Sciences, McMaster University}, {School of Biomedical Sciences, McGill University}, {School of Interdisciplinary Science, McMaster University}}
---

```{r setup, include=FALSE}
library(papaja)
library(kableExtra)
library(tidyverse)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\noindent \textbf{\emph{Public Significance Statement}}  
\noindent A robust Bayesian meta-analysis showed that directing learners to focus their attention on their intended movement effects---often called an external focus---may have little-to-no effect on motor performance and learning on average. While the consensus among researchers and practitioners has been that an external focus is superior to focusing on one's own body during practice, the present results suggest this may depend on unknown factors and our current understanding has been distorted by publication bias.

\newpage

Where should you focus when performing and/or learning a motor skill? The most basic of questions for a novice learner and an experienced performer alike. Is it better to focus on what you are doing: where your body is in space and how it is behaving? Or is it better to focus on what you intend to do: the end effect you are trying to achieve independent of how your body achieves it? This question has been the topic of decades of research comparing an internal focus of attention (i.e., focusing on your own body) to an external focus of attention (i.e., focusing on the intended effect of the action). Gabriele Wulf pioneered this area of inquiry in 1998, publishing a two-experiment paper illustrating the benefits of adopting an external focus [@wulf1998-meta]. In the experiments, instructing learners to focus on the wheels of a ski simulator (Experiment 1) or the markers on a balance platform (Experiment 2) led to improved motor learning compared to focusing on oneâ€™s feet. Dozens of studies have since replicated these initial findings [see @wulf2007; @wulf2013 for reviews].

Previous reviews have argued that research shows benefits of an external focus in four main areas: (a) effectiveness at accuracy and balance tasks, (b) efficiency in electromyographic activity, force production, speed, and endurance tasks, (c) promoting automaticity, and (d) enhancing movement form [@chua2021; @wulf2016; @wulf2013; @wulf2007]. A leading explanation for the mechanism causing these benefits is goal-action coupling: a process proposed in Wulf and Lewthwaite's [-@wulf2016] OPTIMAL theory involving a shift at the neural level that simultaneously directs action toward success and stifles deleterious self-focused cognition. While focus of attention is fundamental to the OPTIMAL theory, various perspectives in motor behavior have offered complementary accounts for external focus benefits. For example, it has been argued from the constraints-based approach that an external focus promotes the search of the task during practice and provides a constraint on emerging actions [@davids2003]. It has also been argued that actions and perceptions share a common (cognitive) code; therefore, focusing on the intended (perceptual) effect of an action is consistent with its underlying neural coding [@wulf2001; @prinz1990; @hommel2001]. While research continues to explore the putative mechanisms, there is consensus in the motor learning community that adopting an external focus of attention can improve motor performance, retention, transfer, and movement efficiency---at least most of the time [@lee2021a; @chua2021; @makaruk2020; @kim2017; @li2022; @grgic2021; @grgic2022; @nicklas2022].[^1]

[^1]: \textcolor{blue}{While we acknowledge this as the general consensus in the field, it is important to note that there are mixed findings and alternative discussions in this area of research} [e.g., @bernier2016; @perkins2003-meta; @maurer2013-meta; @collins2016; @brick2014; @peh2011; @lawrence2011-meta; @zentgraf2009-meta; @emanuel2008-meta; @canning2005; @schorer2012].

Buttressed by the largely positive results in the research literature, external focus of attention is now widely recommended outside of academia, including by sport coaches [skating: @smale2021; golf: @neumann2017; tennis: @kuzdub2022; baseball: @peterson2019], fitness coaches [@winkelman2015; @kompf2015], and therapists [@magne2017; @lo2019]. Researchers continue to study the use of externally focused instructions and feedback in clinical settings [@johnson2023] and are currently developing strategies for increasing awareness of the research among rehabilitation professionals [@hussien2023a; @hussien2023b; @hussien2023]. As external focus becomes evermore mainstream, recent concerns that much of the motor learning literature may be exaggerated by reporting bias [e.g., @lohse2016; @mckay2022b; @mckay2022a; @twomey2021b; @mesquida2022] underlines the need for careful assessment of the evidence. The external focus literature may be especially at risk because substantial reporting bias has been found in the motor learning literature investigating the other factors within OPTIMAL theory [@mckay2022b; @bacelar2022b; @mckay2023]. \textcolor{blue}{Note that reporting bias encompasses various forms of selection bias that limit the availability of data. Potential reporting bias mechanisms can be modeled, though models cannot determine the specific reason for censorship within a literature.}

## Previous meta-analyses

There have been seven meta-analyses comparing the effects of internal and external focus instructions on motor outcomes. Five have focused on specific task-types: balance [@kim2017], jumping [@makaruk2020], sprinting [@li2022], strength [@grgic2021], and endurance [@grgic2022]. A sixth included all motor tasks and focused specifically on the immediate effect on performance [@nicklas2022]. Chua and colleagues [-@chua2021] conducted the most comprehensive meta-analysis of the seven, including all task-types and estimating effects on performance, retention, transfer, electromyography activity, and the distance effect. All seven studies reported the results of random effects meta-analyses as the primary estimates for the effect of focus of attention. Although there was some variance in point estimates and confidence intervals, each of the studies reported evidence that an external focus is superior to an internal focus.

Importantly, a random effects model assumes no reporting bias \textcolor{blue}{and has been shown to be quite biased in the presence of selective reporting for statistical significance} [@bartos2023a; @bom2019; @carter2015; @carter2019; @kvarven2020; @stanley2017; @stanley2022]. Two of the seven previous studies [@chua2021; @kim2017] reported evidence of funnel plot asymmetry, which is consistent with selective reporting of significant results. Two studies did not find evidence of funnel plot asymmetry [@nicklas2022; @li2022], and the other three did not investigate reporting bias at all [@grgic2021; @grgic2022; @makaruk2020]. Both studies that observed evidence of reporting bias conducted a fail-safe-style sensitivity analysis, but did not correct the primary estimates for the presence of bias. The meta-analysis by @chua2021 did calculate worst-case-scenario estimates based on a random effects meta-analysis of the non-significant results. Thus, although reporting bias may be prevalent in the field of motor learning [@lohse2016], and two previous meta-analyses have found evidence of reporting bias in the attentional focus literature [@chua2021; @kim2017], the primary estimates from all previous meta-analyses assume bias is absent.

Consistent with the other studies, @chua2021 reported moderate benefits of an external focus for learning measures (*g* = .58) and small benefits for performance measures (*g* = .26) and the distance effect (*g* = .22). Chua and colleagues also reported a large effect on electromyography activity (*g* = .83). In lieu of bias-corrected estimates, worst-case scenario estimates were calculated to evaluate how sensitive the primary estimates were to an assumed model of reporting bias. Under the assumed model, significant results in the predicted direction are published without censorship, while all non-significant results and significant results in the opposite direction are censored at the same rate. The worst-case scenario is simply the random effects estimate of all the non-preferred outcomes, since a preference for significant results in the predicted direction cannot upwardly bias an estimate if significant results are removed. If the worst-case scenario is positive, then one can conclude that no amount of reporting bias could attenuate the point estimate to the null value. However, this conclusion is only merited if censorship is entirely captured by the assumed model. If other plausible mechanisms of censorship are present, then the assumed model does not hold, and the worst-case scenario estimates can no longer be considered as such.

Although @chua2021 concluded that no amount of reporting bias could attenuate the effect to the null value for any measure (performance, retention, transfer, electromyography, and the distance effect), there are several plausible censorship mechanisms that were unexplored. For example, it is plausible that nearly significant results, often called non-significant trends [@otte2022], were censored less than other non-significant trends. It is also possible that point estimates favoring an internal focus were the least preferred result. If these plausible alternative censorship mechanisms were active in the attentional focus literature, then the random effects estimate of "all non-significant in the predicted direction" results would be positively biased. While @chua2021 concluded that external focus superiority is not sensitive to reporting bias, it remains unknown if that conclusion is sensitive to the form of reporting bias that was assumed.

## Present study

Seven previous meta-analyses provide primary estimates of the potential benefit of an external focus of attention while assuming reporting bias is absent. Given the evidence of reporting bias reported in two of those studies [@chua2021; @kim2017], along with evidence of extensive bias in related literatures [e.g., @lohse2016; @mckay2023], bias-corrected estimates are needed. There are several plausible mechanisms of reporting bias, and the true model is unknowable. Therefore, using a robust Bayesian approach to meta-analysis [@bartos2023], we leveraged Bayesian model-averaging to fit several plausible models of reporting bias to the attentional focus literature examined by @chua2021. Greater weight was given to the models that best accounted for the results and less weight was given to poorly performing models. This approach allowed us to calculate reporting-bias-adjusted estimates for the effect of attentional focus on motor learning, performance, electromyography activity, and for the distance effect. Our approach naturally allowed us to evaluate Chua and colleagues' [-@chua2021] claims that no amount of reporting bias could attenuate the effect to the null value.

In addition to censorship mechanisms, we also explored the role of *post hoc* outcome selection leading to potentially exaggerated estimates. The previous seven meta-analyses either did not specify exactly how outcomes were selected for analysis [@grgic2021; @grgic2022; @kim2017; @makaruk2020; @li2022], excluded studies that had more than one performance measurement unless the measures could be ranked and a primary measure could be selected [@nicklas2022], or selected the outcome positioned as primary in the original research article [@chua2021]. The \textcolor{blue}{external} focus literature has not made use of preregistration or Registered Reports, so it is possible that the most impressive results have sometimes been positioned as primary because they were the most impressive. If this sort of *post hoc* selection is present, then selecting outcomes based on their status in the original article may lead to biased estimates. To evaluate the possibility of *post hoc* selection bias, we extracted effect size estimates for the retention test outcomes that were not selected by @chua2021, but could have been, and compared them to the selected "primary" outcomes.

In the present study, we addressed the following questions: (a) What is the reporting-bias-adjusted estimate for the effect of attentional focus on learning, performance, electromyography activity, and the distance effect? (b) How sensitive are random effects estimates to the assumption that reporting bias is absent? (c) How sensitive are Chua and colleagues' [-@chua2021] conclusions that no amount of reporting bias could attenuate the effect to the null value to the specific model of censorship that was evaluated? and (d) How influential was *post hoc* selection bias on the estimated benefits of an external focus of attention on retention performance?

# Methods

## Transparency and openness

We adhered to the MARS guidelines for meta-analytic reporting [@appelbaum2018]. The data, code, and preregistration for this study can be found here: https://osf.io/vfmx2/?view_only=002325d59dd64562a20301167240f0f9. The data for each primary outcome measure were collected and reported by @chua2021. Our re-analysis of those data was not preregistered as we were already aware of Chua and colleagues' [-@chua2021] primary conclusions and had seen the data visualizations in their study. Data for up to three additional outcomes from each experiment that examined retention test performance were collected and analyzed according to our preregistered protocol. 

Statistical analyses were conducted using `r cite_r("../r-references.bib")` were used in this project.

## Eligibility

Our analysis was restricted to the studies included in the study by @chua2021, meaning our study inherits the inclusion criteria imposed in their study: (a) published in English between February 1998 and April 2019, (b) in a peer-reviewed journal, (c) compared internal and external foci of attention, or at least two types of external focus, (d) measured motor learning or performance, (e) used a within-participant design to measure performance and a between participants design to measure learning, (f) included sufficient data to calculate effect sizes, and (g) were experiments.

## Data collection process

The data reported by @chua2021 were extracted directly from the published article. Additionally, up to three outcomes were extracted from each experiment included in the meta-analysis of retention test performance.[^2] Data were extracted in duplicate by \textcolor{blue}{a team} of six researchers working independently \textcolor{blue}{(AC, JS, CDF, HH, KA, FA)}. The lead author evaluated each pair of extractions using the `R` package `daff` [@R-daff] for consensus and resolved all conflicts.

[^2]: \textcolor{blue}{We chose to focus on retention effects because the performance estimates were already small. The retention estimates were substantial, and retention tests are often the focal learning measure in an experiment. Almost all transfer tests were from studies that also included a retention test, so focusing on retention outcomes was the simplest way to test our research question.}

Outcome measures were selected for extraction based on our preregistered priority list (see Table \@ref(tab:table1)). A priority list achieved two goals. First, it prevented selection bias when several outcomes were reported in a study by establishing which outcomes to select *a priori*. Second, the list prioritized outcomes most connected to the goal of the task over outcomes only correlated with success. This ensured the dependent variables most indicative of goal-action coupling were selected from each study.

```{r table1, echo = FALSE, results = "asis"}
table1 <- tibble::tibble(
  c1 = c(1:5),
  c2 = c("Absolute error",
         "Root mean squared error / Total error",
         "Accuracy points",
         "Variable error",
         "Absolute timing error"),
  c3 = c(6:10),
  c4 = c("Relative timing error",
         "Absolute constant error",
         "Movement time",
         "Movement form (Expert raters)",
         "Other")
)
table1 |>  
  kbl(booktabs = TRUE,
      escape = TRUE,
      #linesep = "\\addlinespace",
      align = c("c", "l", "c", "l"),
      caption = "Priority list for extracting outcome measures.",
      col.names = c("Priority",
                    "Measure",
                    "Priority",
                    "Measure")
  )  |>  
  kable_styling(position = "left",
                font_size = 10
                #latex_options = "scale_down"
                )
```

The sample sizes, direction of effect, means, and standard deviations were extracted for each measure when available. If standard deviations were not reported, data were extracted in the following order of priority: means and standard errors, *F*-values, then *t*-values. If the required data were not reported in the text of the article, but were presented in figures with error bars, then the mean and standard deviation were extracted by digitizing the plots [@rohatgi2022]. \textcolor{blue}{Data from six studies were digitized.} If data could not be extracted with plot digitization, then the authors were emailed, and the data were requested. If the authors did not respond, a follow up email was sent. \textcolor{blue}{Emails were sent to authors requesting data for five effects, and one author responded with the requested data.} Hedges' *g* for the newly extracted outcomes was calculated using the `R` package `compute.es` [@R-compute.es]. Risk of bias from methodological weaknesses was well probed by @chua2021 and was not revisited in this study.

## Synthesis methods

### Influential cases

We screened the data for influential cases using the `R` package `metafor` [@R-metafor]. After fitting univariate random effects models for each meta-analysis, externally standardized residuals and Cookâ€™s distances were calculated. Studies identified as extreme by both measures were considered influential and a sensitivity analysis was conducted with the studies removed.[^3]

[^3]: \textcolor{blue}{Our approach to influential case screening differed from the approach employed by} @chua2021 \textcolor{blue}{and we therefore arrived at a different number of outliers for each analysis (see Supplementary A for more details).}

### Reporting bias

We implemented a robust Bayesian approach [@bartos2023] to reanalyze the five meta-analyses reported by @chua2021. We used neutral default priors for the presence of an effect (\textcolor{blue}{Normal(M = 0, SD = 1),} *p* = .5), the presence of heterogeneity (\textcolor{blue}{InvGamma(1, 0.15),}*p* = .5), and the presence of reporting bias (*p* = .5). Reporting bias was probed using selection models and funnel plot regression models. In the selection model class, six different weight-function models were fit to model censorship based on specific *p*-value thresholds. For example, one selection model captures the possibility that significant results in the predicted direction are more likely to survive to be published than both null results and significant results in the unpredicted direction. Another selection model captures the possibility that results in the unpredicted direction are the least likely to survive censorship, while non-significant trends are more likely than other null results, but not as likely as significant results to survive.

A total of six selection models capturing different plausible censorship scenarios are assigned half of the prior probability that reporting bias exists. The other half of the prior probability is allocated to funnel plot regression models. The precision-effect test (PET) and precision-effect estimate with standard errors (PEESE) respectively model a linear and quadratic relationship between standard error and effect size. If the data were censored such that lower *p*-values had a higher probability of surviving, a correlation would emerge between two otherwise independent causes of *p*-values: effect sizes and standard errors. The PET method fits a linear relationship between effect size and standard error, modeling a consistent level of censorship across studies. The PEESE method fits a quadratic relationship, reflecting the possibility that studies with small standard errors, and thus large samples, are likely to be reported regardless of the results, while small studies with large standard errors require increasingly impressive results to garner publication.[^4] 

[^4]: \textcolor{blue}{Priors for the six selection models were: $\omega$[two-sided: .05] $\sim$ CumDirichlet(1, 1), $\omega$[two-sided: .1, .05] $\sim$ CumDirichlet(1, 1, 1), $\omega$[one-sided: .05] $\sim$ CumDirichlet(1, 1), $\omega$[one-sided: .05, .025] $\sim$ CumDirichlet(1, 1, 1), $\omega$[one-sided: .5, .05] $\sim$ CumDirichlet(1, 1, 1), $\omega$[one-sided: .5, .05, .025] $\sim$ CumDirichlet(1, 1, 1, 1). Priors for the two regression models were: PET $\sim$ Cauchy(0, 1)[0, Inf], PEESE $\sim$ Cauchy(0, 5)[0, Inf].} 

A total of 36 models were fit to the data with every combination of the eight reporting bias models, models assuming an effect, no effect, heterogeneity, no heterogeneity, and no reporting bias \textcolor{blue}{(see Supplementary B for more details)}. The estimates of each model were combined using Bayesian model-averaging, where model estimates are weighted based on how well the model fit the data. A single posterior distribution was generated for the average effect of an external focus and the average value of *tau*---the estimated heterogeneity. Further, Bayes Factors were calculated measuring the evidence in favor of an effect, the presence of heterogeneity, and reporting bias. 

### Post-hoc selection bias

A multi-level mixed effects model with outcomes nested in study, and with cluster-robust standard errors compared the outcomes selected by @chua2021 to the additional outcomes that might have been selected instead. Profile analysis was conducted to ensure the model converged on unique solutions for estimates of *Mu* \textcolor{blue}{(the mean effect of external focus in the population)} and *tau*.

# Results

```{r echo=FALSE}
source("../../scripts/results_summary-for-paper.R")
```

Model-averaged posterior distributions for each analysis with and without outliers are presented in Figure \@ref(fig:fig1).[^5]

[^5]: \textcolor{blue}{Model convergence diagnostics were conducted for all RoBMA analyses. In each case, Rhat convergence values were less than 1.05 and effect sampling sizes were a few hundred or more.}

## Performance

Influence analyses revealed four studies \textcolor{blue}{(Marchant, Greig, et al., 2009; Nadzalan et al., 2015; Porter, Nolan, et al., 2010; Sherwood et al., 2014; Exp 1 and 2)} could be considered outliers in the performance meta-analysis. We report the results with all studies included first, then with outliers removed. The mean of the model-averaged posterior distribution for the difference between external and internal foci of attention on motor skill performance was *g* = `r performance_mean`, 95% credible interval: `r performance_lb`, `r performance_ub`. The data were over 5 times more compatible with the null hypothesis than the alternative, BF~10~ = `r performance_BF10`. There was clear evidence of heterogeneity, $\tau$ = `r performance_tau`, BF~rf~ = Infinite. There was also clear evidence of publication bias, BF~pb~=`r performance_BFpb`. Removing influential cases did not substantively change the conclusions: *g* = `r performance2_mean`, 95% credible interval: `r performance2_lb`, `r performance2_ub`, BF~10~ =`r performance2_BF10`; $\tau$ =`r performance2_tau`, BF~rf~ = `r performance2_BFrf`; BF~pb~ = `r performance2_BFpb`.


\clearpage

```{r fig1, echo = FALSE, fig.cap = "(ref:fig1-caption)", fig.align = "center"}
knitr::include_graphics("../../figs/fig1.pdf")
```

(ref:fig1-caption) \linespread{1.15}\selectfont \small \normalfont \textbf{Posterior plots of the standardized mean difference with and without outliers.} The effect size estimates (*g*) of each meta-analysis with all studies included (top row) and with outliers removed (bottom row). The histograms in the first column reflect the prior distribution, with 50% of the probability density concentrated on zero effect (the null hypothesis) and 50% of the density normally distributed (*M* = 0, *SD* = 1). The model-averaged posterior distributions for performance, retention, transfer, electromyography, and the distance effect are presented in the second through sixth columns, respectively. Increased belief in the null hypothesis is visible for each analysis, illustrated by the increased height of the spike at *g* = 0 in all posteriors relative to the prior distribution.

\clearpage

## Retention
Two studies \textcolor{blue}{(Ahmad et al. 2013; Tse, 2017)} were identified as possible outliers in the retention test meta-analysis. Again, the results with all studies included are reported first, then with outliers removed. The mean of the model-averaged posterior distribution for the effect of focus of attention on retention was *g* = `r retention_mean`, 95% credible interval: `r retention_lb`, `r retention_ub`. The data were somewhat more consistent with the null hypothesis than the alternative, BF~10~ = `r retention_BF10`. There was clear evidence of heterogeneity, $\tau$ = `r retention_tau`, BF~rf~ = Infinite. The data were  5.9 times more compatible with models assuming publication bias than without, BF~pb~ = `r retention_BFpb`. Removing two influential cases did not substantively change the conclusions: *g* = `r retention2_mean`, 95% credible interval: `r retention2_lb`, `r retention2_ub`, BF~10~=`r retention2_BF10`; $\tau$ =`r retention2_tau`, BF~rf~ = `r retention2_BFrf`; BF~pb~ = `r retention2_BFpb`.

## Transfer

One possible outlier \textcolor{blue}{(Tse, 2017)} was identified in the transfer test meta-analysis. The mean of the model-averaged posterior distribution of all transfer outcomes was *g* = `r transfer_mean`, 95% credible interval: `r transfer_lb`, `r transfer_ub`. The results were somewhat more likely under the null hypothesis than the alternative, BF~10~ = `r transfer_BF10`. There was clear evidence of heterogeneity, $\tau$ = `r transfer_tau`, BF~rf~ = Infinite. The data were more than 6.4 times more likely under models assuming publication bias, BF~pb~ = `r transfer_BFpb`. Removing one influential case did not substantively change the conclusions: *g* = `r transfer2_mean`, 95% credible interval: `r transfer2_lb`, `r transfer2_ub`, BF~10~ = `r transfer2_BF10`; $\tau$ = `r transfer2_tau`, BF~rf~ = `r transfer2_BFrf`; BF~pb~ = `r transfer2_BFpb`.

## Electromyography

There were no outliers identified in the electromyography meta-analysis. The mean of the model-averaged posterior distribution for the effect of attentional focus on electromyography activity was *g* = `r emg_mean`, 95% credible interval: `r emg_lb`, `r emg_ub`. The data were twice as likely under the null hypothesis as the alternative, BF~10~ = `r emg_BF10`. There was clear evidence of heterogeneity, $\tau$ = `r emg_tau`, BF~rf~ = Infinite. There was very strong evidence of publication bias, BF~pb~ = `r emg_BFpb`.

## Distance effect

One possible outlier \textcolor{blue}{(Lohse et al., 2014)} was identified in the distance effect meta-analysis. The mean of the model-averaged posterior distribution for the difference between distal and proximal external foci was *g* = `r distance_mean`, 95% credible interval: `r distance_lb`, `r distance_ub`. The results were over 3.8 times more likely under the null hypothesis than the alternative, BF~10~ = `r distance_BF10`. There was clear evidence of heterogeneity, $\tau$ = `r distance_tau`, BF~rf~ = `r distance_BFrf`. There was overwhelming evidence of publication bias, BF~pb~ = `r distance_BFpb`. Removing the influential case did not substantively change the conclusions: *g* = `r distance2_mean`, 95% credible interval: `r distance2_lb`, `r distance2_ub`, BF~10~ = `r distance2_BF10`; $\tau$ = `r distance_tau`, BF~rf~ = `r distance2_BFrf`; BF~pb~ = `r distance2_BFpb`.

## Selection moderator

Outcomes selected for inclusion in Chua and colleagues' [-@chua2021] meta-analysis of retention performance were somewhat larger (*g* = `r selected_mean`, 95% confidence interval: `r selected_lb`, `r selected_ub`) than the additional outcomes that could have been extracted but were not (*g* = `r not_selected_mean`, 95% confidence interval: `r not_selected_lb`, `r not_selected_ub`; see Figure \@ref(fig:fig2)). However, the difference between selected and not-selected outcomes was not statistically significant, *F*(`r qdf1`, `r qdf2`) = `r q`, *p* = `r q_p`.

\clearpage
\vspace{-3em}

```{r fig2, echo = FALSE, fig.cap = "(ref:fig2-caption)", fig.align = "center", out.height = "71.5%"}
knitr::include_graphics("../../figs/fig2.pdf")
```

(ref:fig2-caption) \linespread{1.15}\selectfont \small \normalfont \textbf{Forest plot of retention outcomes separated by "selected" moderator.} Standardized mean difference (*g*) and 95% confidence intervals for each study included in the meta-analysis of retention outcomes. The green polygon represents the mean and 95% confidence interval for outcomes that @chua2021 selected for analysis. The purple polygon represents the estimate for outcomes reported in the original experiments but were not selected by @chua2021. The error bars extending from both polygons reflect the 95% prediction interval, illustrating the range of outcomes we would expect to observe in 95% of studies randomly sampled from the same population of studies included in this analysis. The prediction intervals account for the substantial unexplained heterogeneity present in these data, showing that even without correcting for publication bias we would expect outcomes across the entire plausible range of effects.

\clearpage

## Individual model fit

As implied by the results of each analysis, the best performing models overall assumed heterogeneity, publication bias, and zero effect (see Figure \@ref(fig:fig3)). The best fitting publication bias models were the PET and PEESE funnel plot regression models, as well as the selection models that assumed directional hypotheses, particularly those that modeled censorship based on the direction of the point estimate. This pattern of findings suggests complex, results-based selection mechanisms linked to more than just statistical significance.

\clearpage

```{r fig3, echo = FALSE, fig.cap = "(ref:fig3-caption)", fig.align = "center", out.width="99%"}
knitr::include_graphics("../../figs/fig3.pdf")
```

(ref:fig3-caption) \linespread{1.15}\selectfont \small \normalfont \textbf{Total Inclusion Bayes Factor for each model relative to the ensemble, summed across each of the five analyses with and without outliers.} Higher Inclusion Bayes Factors indicate better agreement with the data than the average of the ensemble. The green circles represent naÃ¯ve fixed and random effects models that assume no publication bias. The purple circles represent six selection models and two regression models, each modeling publication bias in a different way. A figure illustrating each of the publication bias models is displayed below the lollipop plot, shown in the same left-to-right order they follow in the plot above. The size of each circle reflects the prior probability assigned to the model (*p* = .125 for naÃ¯ve models, *p* = .031 for regression models, and *p* = .01 for selection models). The naÃ¯ve and publication bias models were fit testing four scenarios: (a) an effect is present, no heterogeneity, (b) an effect is present, heterogeneity is present, (c) an effect is absent, no heterogeneity, and (d) an effect is absent, heterogeneity is present. The PEESE model, presented on the far right in each scenario, dominated the other models when assuming an effect is absent and heterogeneity is present. To better illustrate the performance of each model in the ensemble, Inclusion Bayes Factors are shown on a log scale on the *y*-axis.

\clearpage


# Discussion

We re-evaluated the evidence in support of an external focus benefit for learning, performance, muscular efficiency, and the distance effect. Seven previous meta-analyses have relied on the results of naÃ¯ve random effects models that assume zero reporting bias in the primary estimates. \textcolor{blue}{However, it has become clear that such an assumption may not be appropriate for motor learning research} [@mckay2022b; @mckay2023]. Each of the previous seven meta-analyses concluded that an external focus is superior to an internal focus. @kim2017 reported the benefits applied to balance learning, performance, and transfer. @makaruk2020 found the same for jump performance and @li2022 reported similar results for sprint performance. Grgic and colleagues reported external focus benefits for both muscular strength and endurance [@grgic2021; @grgic2022]. @nicklas2022 reported the advantage of an external focus over an internal focus applied to immediate performance in general. The most comprehensive of the meta-analyses, and the study whose data we reanalyzed, was conducted by @chua2021. They estimated small to moderate benefits for each specific effect and concluded that no amount of publication bias could attenuate the observed effects to zero.

Our results differ from previous \textcolor{blue}{meta-anayses} as reporting bias was \textcolor{blue}{unaccounted for} in their primary estimates. \textcolor{blue}{This is a serious limitation of the previous external focus meta-analyses as simulation studies have clearly demonstrated that random-effects result in large biases and high rates of false positives in the presence of publication-selection bias} [@bartos2023a; @bom2019; @stanley2017; @stanley2022]\textcolor{blue}{, which have been further supported when random-effects are compared with preregistered multilab replications} [@kvarven2020]\textcolor{blue}{.} We therefore explicitly modeled bias and estimated trivially small effects in each analysis. While @chua2021 concluded that no amount of publication bias could reduce the effects to the null, our models suggest the data favor the null hypothesis for each analysis. If the only type of reporting bias in the literature is one-sided selection at *p* = .05, then Chua and colleagues' conclusions were justified. However, if there were other considerations, such as sample size, trends, and direction of point estimates, the assumptions of their model were violated. Our analysis suggests this is the case for the focus of attention literature. \textcolor{blue}{Thus, similar to previous simulation studies our findings illustrate that reporting bias can cause random effects models to produce even large effect estimates when the true model is null. The random effects estimates reported by} [@chua2021] \textcolor{blue}{ranged from small to large, while our corrected estimates range from essentially nil to trivial at best.}

Although we observed somewhat larger estimates among effects selected by @chua2021 than among alternative outcomes that could have been selected, the difference was small and easily attributable to chance. The stronger signal for selection came from censorship prior to appearing in the published sample. Thus, the average reader of this literature would not have been inoculated against bias by having access to the complete results of each paper. The biasing influence of censorship would have already affected the sample of information readers could access.

These findings underscore uncertainty about external focus benefits. Adding to this uncertainty, we observed significant unexplained heterogeneity in effects. This heterogeneity could imply that focus of attention has a range of effects that depend on situational factors. If so, our results suggest that an internal focus may be superior to an external focus in nearly as many situations as the reverse. Alternatively, this heterogeneity may be due to methodological idiosyncrasies, unmodeled selection, or poor data curation at any level. As with censorship mechanisms, we have no way to know which potential sources of heterogeneity were at play.

Unfortunately, the present results add to a growing body of metascience questioning the extant support for the predictions in OPTIMAL theory [see @mckay2023 for a recent meta-analysis on the other two pillars in the theory]. In addition to predicting external focus benefits for learning and performance, OPTIMAL theory also predicts beneficial effects for autonomy and enhanced expectancies via similar underlying mechanisms [@wulf2016]. The primary corpus of evidence supporting motor learning benefits from autonomy is the self-controlled practice literature. Self-controlled practice involves asking learners to choose an aspect of their practice environment and the published literature suggests this will confer noticeable benefits to performance and learning [for a review see @stemarie2020]. However, like the external focus literature, the self-controlled practice research shows substantial evidence of reporting bias and more support for the null hypothesis [@mckay2022b]. Approximately the same pattern emerges for the enhanced expectancies research [@bacelar2022b]. While the published literature appears to unequivocally demonstrate the predicted motor benefits of enhancing a learnerâ€™s expectancy for success, accounting for reporting bias suggests uncertainty and heterogeneity [@mckay2023]. Taken together, this meta-evidence suggests the underlying mechanism common to all three factors of the tripartite OPTIMAL theory may be censorship. The mechanisms forwarded in OPTIMAL theory are made no less valid by this conclusion; it is the evidence rather than the theory that has been impugned by this body of work.[^6]

[^6]: \textcolor{blue}{This conclusion also applies to other theories and perspectives (e.g., ecological dynamics) that have been forwarded based on the extant attentional focus literature to account for a supposed external focus advantage} [e.g., @davids2003; @wulf2001; @prinz1990; @hommel2001; @gottwald2023].

## Limitations

The evidence in the review contains small sample sizes and small to moderate risk of bias according to Chua and colleagues [-@chua2021]. None of the studies were preregistered. There were 20 studies missing due to insufficient information to calculate effect sizes in the original data set and another four missing effects from our extraction of secondary outcomes.

We did not explore whether manipulation checks verified that the instructed attentional focus was adopted during performance. OPTIMAL theory predicts that when learners focus on their intended effect on the environment, they facilitate goal-action coupling, benefiting learning and performance. Our analysis only investigated whether instructions \textcolor{blue}{or feedback} impacted performance. Perhaps a missing moderator in our analysis was the extent to which focus instructions were followed in each experiment. We chose not to explore this possibility because there are no validated manipulation checks.

## Recommendations and conclusions

The potential benefit of adopting an external focus of attention is among the most important contributions of academic motor learning research. It fits with numerous theoretical perspectives in the scientific literature and has been widely promoted in an array of applied settings, including sports, rehabilitation, and education. Our findings impugn the evidential basis for the superiority of an external focus of attention. However, rather than establishing nil or trivial benefits from focusing externally, uncertainty remains. The posteriors include interesting effects, there may be important moderators, and our estimates may have overcorrected for bias. We simply do not know if an external focus provides meaningful benefits to motor learning and performance or not. \textcolor{blue}{Moving forward, it will therefore be critical for researchers conducting meta-analyses in motor learning and related areas (e.g., psychology, neuroscience, sport and exercise science) to adopt the use of leverage statistics as the default approach for identifying outliers} [see @deeks2023; @viechtbauer2010a for discussions]\textcolor{blue}{.}

Building knowledge about external focus effects can be accelerated by adoption of the Registered Report publication format [@chambers2019]. Registered Reports prevent publication bias [@scheel2021], and when they include preregistration of analysis plans, they prevent *p*-hacking [@simmons2011] and HARKing [@kerr1998] as well. Limited resources may prevent individual laboratories from collecting sufficient sample sizes for a well-powered Registered Report, so researchers are encouraged to collaborate extensively to achieve the sample sizes necessary to make progress.

\newpage

# References

References marked with an asterisk (*) indicate studies included in the meta-analysis.

\vspace{2ex}
::: {#refs custom-style="Bibliography"}
:::

